{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify fraud in Enron corpus\n",
    "\n",
    "## by Aarthy  Vallur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Enron corpus\n",
    "The Enron corpus is a large database of over 600,000 emails from and to 156 employees of the Enron Corporation. The dtabase was collected over a 2 week period in May, 2002 by a contractor hired by the FERC which was investigating the causes and frauds in the company's sudden bankruptcy in Dec, 2001. The corpus is unique in that it is one of the few publicly available email data sets, as similar datasets are usually rendered unusable due to stringent privacy laws and controls.\n",
    "The Enron corpus was a window into the widespread financial and and trading scandals that plagued Enron Corporation prior to its collapse which bankrupted thousands of share holders in 2001. The entire scandal and the chief executives implicated in the scandal are detailed at [Enron Corporation scandal](https://en.wikipedia.org/wiki/Enron). The term \"person of interest\" or \"poi\" (as depicted in the corpus) was used to tentatively identify those employees with a high chance of playing an active part in the frauds that preceeded Enron's collapse. Many features, including their financial, stock and email records were used to identify those that could be considered \"poi\"s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "The dataset provided of the Enron employees, includes financial, stock and email records of 146 employees at Enron. Our task is to employ data cleaning and machine learning techniques to identify pois given this information. The goal is to come up with an algorithm with precision and recall > 0.3 that can reasonably identify those employees that have a high chance of being pois. Key features of the dataset are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loading the packages and codes necessary\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from time import time\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import preprocessing \n",
    "import tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 146 entries in the enron email file\n",
      "There are 21 features for each person in the enron email datafile and they are\n",
      "['salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'email_address', 'from_poi_to_this_person']\n"
     ]
    }
   ],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "## Exploring data \n",
    "print \"There are\", len(data_dict), \"entries in the enron email file\"\n",
    "## Total number of features \n",
    "all_features =  data_dict['METTS MARK'].keys()\n",
    "print \"There are\", len(data_dict['METTS MARK']), \"features for each person in the enron email datafile and they are\"\n",
    "print all_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 146 individuals in the given dataset with each having information grouped under 21 features. Of these,'salary', 'deferral_payments', 'total_payments', expenses', 'long_term_incentive', 'loan_advances', 'director_fees' and 'deferred_income' refer to financial trasactions. While, 'exercised_stock_options', 'restricted_stock', 'restricted_stock_deferred' and 'total_stock_value'refer to stock trasactions. Email transactions are represented by 'to_messages', 'shared_receipt_with_poi', 'from_messages', 'from_this_person_to_poi' and 'from_poi_to_this_person'. Descriptive feature with no bearing on poi status are 'email_address' while 'other' is ill- defined. 'poi' refers to a pre- existing label which will be further explored in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\". \n",
    "### making feature list with poi as the label. Chose features that are likely to matter for pois, including financial and email related features. \n",
    "### \"email address\" was not chosen since it is descriptive and \"other\" was not chosen as it is ill- defined\n",
    "features_list = ['poi', 'salary', 'total_payments', 'exercised_stock_options', 'bonus', 'total_stock_value', 'long_term_incentive', 'shared_receipt_with_poi', 'from_this_person_to_poi', 'from_poi_to_this_person', 'restricted_stock', 'restricted_stock_deferred', 'deferred_income', 'director_fees', 'expenses']\n",
    "## There are financial and email related features of which total_payments and total_stock_value are aggregate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 pois in the datafile and they are ['HANNON KEVIN P', 'COLWELL WESLEY', 'RIEKER PAULA H', 'KOPPER MICHAEL J', 'SHELBY REX', 'DELAINEY DAVID W', 'LAY KENNETH L', 'BOWEN JR RAYMOND M', 'BELDEN TIMOTHY N', 'FASTOW ANDREW S', 'CALGER CHRISTOPHER F', 'RICE KENNETH D', 'SKILLING JEFFREY K', 'YEAGER F SCOTT', 'HIRKO JOSEPH', 'KOENIG MARK E', 'CAUSEY RICHARD A', 'GLISAN JR BEN F']\n",
      "CHAN RONNIE have NaN for salary, bonus, total payments and exercised stock options\n",
      "CLINE KENNETH W have NaN for salary, bonus, total payments and exercised stock options\n",
      "POWERS WILLIAM have NaN for salary, bonus, total payments and exercised stock options\n",
      "PIRO JIM have NaN for salary, bonus, total payments and exercised stock options\n",
      "LOCKHART EUGENE E have NaN for salary, bonus, total payments and exercised stock options\n",
      "HAYSLETT RODERICK J have NaN for salary, bonus, total payments and exercised stock options\n",
      "Features and values for Chan Ronnie are {'salary': 'NaN', 'to_messages': 'NaN', 'deferral_payments': 'NaN', 'total_payments': 'NaN', 'exercised_stock_options': 'NaN', 'bonus': 'NaN', 'restricted_stock': 32460, 'shared_receipt_with_poi': 'NaN', 'restricted_stock_deferred': -32460, 'total_stock_value': 'NaN', 'expenses': 'NaN', 'loan_advances': 'NaN', 'from_messages': 'NaN', 'other': 'NaN', 'from_this_person_to_poi': 'NaN', 'poi': False, 'director_fees': 98784, 'deferred_income': -98784, 'long_term_incentive': 'NaN', 'email_address': 'NaN', 'from_poi_to_this_person': 'NaN'}\n",
      "Features and values for Kenneth Cline are {'salary': 'NaN', 'to_messages': 'NaN', 'deferral_payments': 'NaN', 'total_payments': 'NaN', 'exercised_stock_options': 'NaN', 'bonus': 'NaN', 'restricted_stock': 662086, 'shared_receipt_with_poi': 'NaN', 'restricted_stock_deferred': -472568, 'total_stock_value': 189518, 'expenses': 'NaN', 'loan_advances': 'NaN', 'from_messages': 'NaN', 'other': 'NaN', 'from_this_person_to_poi': 'NaN', 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'NaN', 'from_poi_to_this_person': 'NaN'}\n",
      "Features and values for William Powers are {'salary': 'NaN', 'to_messages': 653, 'deferral_payments': 'NaN', 'total_payments': 'NaN', 'exercised_stock_options': 'NaN', 'bonus': 'NaN', 'restricted_stock': 'NaN', 'shared_receipt_with_poi': 12, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 'NaN', 'expenses': 'NaN', 'loan_advances': 'NaN', 'from_messages': 26, 'other': 'NaN', 'from_this_person_to_poi': 0, 'poi': False, 'director_fees': 17500, 'deferred_income': -17500, 'long_term_incentive': 'NaN', 'email_address': 'ken.powers@enron.com', 'from_poi_to_this_person': 0}\n",
      "Features and values for Jim Piro are {'salary': 'NaN', 'to_messages': 58, 'deferral_payments': 'NaN', 'total_payments': 'NaN', 'exercised_stock_options': 'NaN', 'bonus': 'NaN', 'restricted_stock': 47304, 'shared_receipt_with_poi': 3, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 47304, 'expenses': 'NaN', 'loan_advances': 'NaN', 'from_messages': 16, 'other': 'NaN', 'from_this_person_to_poi': 1, 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'jim.piro@enron.com', 'from_poi_to_this_person': 0}\n",
      "Features and values for Eugene Lockhart are {'salary': 'NaN', 'to_messages': 'NaN', 'deferral_payments': 'NaN', 'total_payments': 'NaN', 'exercised_stock_options': 'NaN', 'bonus': 'NaN', 'restricted_stock': 'NaN', 'shared_receipt_with_poi': 'NaN', 'restricted_stock_deferred': 'NaN', 'total_stock_value': 'NaN', 'expenses': 'NaN', 'loan_advances': 'NaN', 'from_messages': 'NaN', 'other': 'NaN', 'from_this_person_to_poi': 'NaN', 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'NaN', 'from_poi_to_this_person': 'NaN'}\n",
      "Features and values for Roderick Hayslett are {'salary': 'NaN', 'to_messages': 2649, 'deferral_payments': 'NaN', 'total_payments': 'NaN', 'exercised_stock_options': 'NaN', 'bonus': 'NaN', 'restricted_stock': 346663, 'shared_receipt_with_poi': 571, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 346663, 'expenses': 'NaN', 'loan_advances': 'NaN', 'from_messages': 1061, 'other': 'NaN', 'from_this_person_to_poi': 38, 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'rod.hayslett@enron.com', 'from_poi_to_this_person': 35}\n"
     ]
    }
   ],
   "source": [
    "## Exploring the names in data_dict with respect to poi\n",
    "count_poi = 0\n",
    "poi_list = []\n",
    "for user in data_dict:\n",
    "    if data_dict[user][\"poi\"] == True:\n",
    "        count_poi+=1\n",
    "        poi_list.append(user)\n",
    "print \"There are\", count_poi, \"pois in the datafile and they are\", poi_list\n",
    "## Looking for persons with NaNs in salary, bonus, total payments and exercised stock options as these are important features\n",
    "for user in data_dict:\n",
    "        if data_dict[user][\"salary\"] == 'NaN' and data_dict[user][\"bonus\"] == 'NaN' and data_dict[user][\"total_payments\"] == 'NaN' and data_dict[user][\"exercised_stock_options\"] == 'NaN' and data_dict[user][\"total_stock_value\"]:\n",
    "            print user, \"have NaN for salary, bonus, total payments and exercised stock options\"\n",
    "## 6 names had a bunch of NaN values for the major feaures chosen. Exploring all six for all features           \n",
    "print \"Features and values for Chan Ronnie are\", data_dict[\"CHAN RONNIE\"]   \n",
    "print \"Features and values for Kenneth Cline are\", data_dict[\"CLINE KENNETH W\"]           \n",
    "print \"Features and values for William Powers are\", data_dict[\"POWERS WILLIAM\"]           \n",
    "print \"Features and values for Jim Piro are\", data_dict[\"PIRO JIM\"]           \n",
    "print \"Features and values for Eugene Lockhart are\", data_dict[\"LOCKHART EUGENE E\"]           \n",
    "print \"Features and values for Roderick Hayslett are\", data_dict[\"HAYSLETT RODERICK J\"]\n",
    "## Apart from Eugene Lockhart, all others have large values for restricted stocks, director's fees and deferred payments. Eugene Lockhart has NaN for all feaures and thus qualifies as an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were 18 individuals identified as pois. For 6 individuals, the aggregate functions had a value of \"NaN\", making them possible outliers with no useful information. Upon closer examination, except for one, the other 5 did have meaningful enties for other fiancial, email and stock features but for Eugene Lockhart, all values were NaN, which makes him a clear candidate for removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 persons based on salary and bonus are: ['LAY KENNETH L', 'SKILLING JEFFREY K', 'TOTAL']\n",
      "Salary and bonus for Total are 26704229 and 97343619\n",
      "Salary and bonus for Kenneth Lay are 1072321 and 7000000\n",
      "Salary and bonus for Jeffrey Skilling are 1111258 and 5600000\n",
      "There are 9 persons who received total payments > 2500000 and total stock options > 5000000. They are ['BAXTER JOHN C', 'HORTON STANLEY C', 'LAVORATO JOHN J', 'WHALLEY LAWRENCE G', 'LAY KENNETH L', 'SKILLING JEFFREY K', 'TOTAL', 'FREVERT MARK A', 'PAI LOU L']\n",
      "People with the highest salary are [('WHALLEY LAWRENCE G', 510364), ('LAY KENNETH L', 1072321), ('SKILLING JEFFREY K', 1111258), ('PICKERING MARK R', 655037), ('TOTAL', 26704229), ('FREVERT MARK A', 1060932)]\n",
      "These 2 people making a salary >500000 are also pois ['LAY KENNETH L', 'SKILLING JEFFREY K']\n",
      "People with the highest bonus are [('KITCHEN LOUISE', 3100000), ('LAVORATO JOHN J', 8000000), ('WHALLEY LAWRENCE G', 3000000), ('DELAINEY DAVID W', 3000000), ('LAY KENNETH L', 7000000), ('BELDEN TIMOTHY N', 5249999), ('SKILLING JEFFREY K', 5600000), ('TOTAL', 97343619), ('ALLEN PHILLIP K', 4175000)]\n",
      "These 4 people making a bonus >3000000 are also pois ['DELAINEY DAVID W', 'LAY KENNETH L', 'BELDEN TIMOTHY N', 'SKILLING JEFFREY K']\n",
      "People with the highest total stock value are [('BAXTER JOHN C', 10623258), ('ELLIOTT STEVEN', 6678735), ('HANNON KEVIN P', 6391065), ('HORTON STANLEY C', 7256648), ('WALLS JR ROBERT H', 5898997), ('LAVORATO JOHN J', 5167144), ('BANNANTINE JAMES M', 5243487), ('WHALLEY LAWRENCE G', 6079137), ('LAY KENNETH L', 49110078), ('REDMOND BRIAN L', 7890324), ('RICE KENNETH D', 22542539), ('OVERDYKE JR JERE C', 7307594), ('SKILLING JEFFREY K', 26093672), ('KEAN STEVEN J', 6153642), ('TOTAL', 434509511), ('WHITE JR THOMAS E', 15144123), ('CHRISTODOULOU DIOMEDES', 6077885), ('DIMICHELE RICHARD G', 8317782), ('YEAGER F SCOTT', 11884758), ('HIRKO JOSEPH', 30766064), ('DERRICK JR. JAMES V', 8831913), ('FREVERT MARK A', 14622185), ('PAI LOU L', 23817930), ('IZZO LAWRENCE L', 5819980)]\n",
      "These 6 people holding a total stock value >5000000 are also pois ['HANNON KEVIN P', 'LAY KENNETH L', 'RICE KENNETH D', 'SKILLING JEFFREY K', 'YEAGER F SCOTT', 'HIRKO JOSEPH']\n"
     ]
    }
   ],
   "source": [
    "## More analysis of bivariate relationships in data_dict, using salary and bonus\n",
    "## Analysing for users with high salary and bonus\n",
    "high_salary_bonus_list = []\n",
    "for user in data_dict:\n",
    "    if data_dict[user][\"salary\"] != 'NaN' and data_dict[user][\"salary\"] >= 1000000 and data_dict[user][\"bonus\"] != 'NaN' and data_dict[user][\"bonus\"] >= 5000000:\n",
    "        high_salary_bonus_list.append(user)\n",
    "print \"Top 3 persons based on salary and bonus are:\", high_salary_bonus_list\n",
    "## Only 3 users have a salary> 1000000 and bonus > 5000000. Of these 'Total\" does not seem to be a real person and is possibly an outlier.\n",
    "## It could be a data entry error. If so, the values for 'Total\" will be orders variant from the 2 next highest users. Verifying this\n",
    "print \"Salary and bonus for Total are\", data_dict[\"TOTAL\"][\"salary\"], \"and\", data_dict[\"TOTAL\"][\"bonus\"]\n",
    "print \"Salary and bonus for Kenneth Lay are\", data_dict[\"LAY KENNETH L\"][\"salary\"], \"and\", data_dict[\"LAY KENNETH L\"][\"bonus\"]\n",
    "print \"Salary and bonus for Jeffrey Skilling are\", data_dict[\"SKILLING JEFFREY K\"][\"salary\"], \"and\", data_dict[\"SKILLING JEFFREY K\"][\"bonus\"]\n",
    "## Clearly, Total seems to be a calculation and data processing error as it is many magnitudes higher than the next 2 hihest values. \n",
    "## More analysis of bivariate relationships in data_dict, this time using the aggregate features, total payments  and total stock value\n",
    "## Analysing for users with high total payments and stock value\n",
    "high_payment_stock_list = []\n",
    "for user in data_dict:\n",
    "    if data_dict[user][\"total_payments\"] != 'NaN' and data_dict[user][\"total_payments\"] >= 2500000 and data_dict[user][\"total_stock_value\"] != 'NaN' and data_dict[user][\"total_stock_value\"] >= 5000000:\n",
    "        high_payment_stock_list.append(user)\n",
    "print\"There are\", len(high_payment_stock_list), \"persons who received total payments > 2500000 and total stock options > 5000000. They are\", high_payment_stock_list\n",
    "## The names on this list look real. No outliers here.\n",
    "## Now checking how high salary, bonus and total stock options correlate with poi status\n",
    "high_salary_list = []\n",
    "high_bonus_list = []\n",
    "high_total_stock_list = []\n",
    "for user in data_dict:\n",
    "    if data_dict[user][\"salary\"] != 'NaN' and data_dict[user][\"salary\"] >= 500000:\n",
    "        high_salary_list.append((user, int(data_dict[user][\"salary\"])))       \n",
    "print \"People with the highest salary are\", high_salary_list\n",
    "high_salary_list_poi = []\n",
    "for user in high_salary_list:\n",
    "    if data_dict[user[0]][\"poi\"] == True:\n",
    "        high_salary_list_poi. append(user[0])\n",
    "print \"These\", len(high_salary_list_poi),\"people making a salary >500000 are also pois\", high_salary_list_poi\n",
    "## 2 of the people with highest salary are also pois.\n",
    "## Repeating the same for bonus and total stock options\n",
    "for user in data_dict:\n",
    "    if data_dict[user][\"bonus\"] != 'NaN' and data_dict[user][\"bonus\"] >= 3000000:\n",
    "        high_bonus_list.append((user, int(data_dict[user][\"bonus\"])))       \n",
    "print \"People with the highest bonus are\", high_bonus_list\n",
    "high_bonus_list_poi = []\n",
    "for user in high_bonus_list:\n",
    "    if data_dict[user[0]][\"poi\"] == True:\n",
    "        high_bonus_list_poi. append(user[0])\n",
    "print \"These\", len(high_bonus_list_poi), \"people making a bonus >3000000 are also pois\", high_bonus_list_poi \n",
    "for user in data_dict:\n",
    "    if data_dict[user][\"total_stock_value\"] != 'NaN' and data_dict[user][\"total_stock_value\"] >= 5000000:\n",
    "        high_total_stock_list.append((user, int(data_dict[user][\"total_stock_value\"])))       \n",
    "print \"People with the highest total stock value are\", high_total_stock_list\n",
    "high_total_stock_list_poi = []\n",
    "for user in high_total_stock_list:\n",
    "    if data_dict[user[0]][\"poi\"] == True:\n",
    "        high_total_stock_list_poi. append(user[0])\n",
    "print \"These\", len(high_total_stock_list_poi), \"people holding a total stock value >5000000 are also pois\", high_total_stock_list_poi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analyses show the overlap between having high salary, bonus and total stock value and being pois. These are features that will be very useful in predicting pois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal\n",
    "As identified in the data set analysis above, 3 clear outliers can be removed to clean up the data set. They are\n",
    "1. Travel Agency in the park- Clearly not an emploee\n",
    "2. Eugene Lockhart- All features are NaN\n",
    "3. Total- Seems to be a spreadsheet error\n",
    "These were removed from the data dictionary to create an outlier- free data for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 143 users in the enron datafile after removing the outliers\n"
     ]
    }
   ],
   "source": [
    "### Outlier removal\n",
    "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\")\n",
    "data_dict.pop(\"LOCKHART EUGENE E\")\n",
    "data_dict.pop(\"TOTAL\", 0)\n",
    "print \"There are\", len(data_dict), \"users in the enron datafile after removing the outliers\"\n",
    "## Cleaned data is ready for storing data-dict as my_dataset as required for grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of new feature\n",
    "Given that the financial and stock- based features have an aggregated feature whch seems to be very meaningful, I thought auch an aggregate feature for the emails will be useful as a new featrure. So, I created an aggregate feature summing the fraction of emails a person received from a poi or sent to a poi. The rationale was that, any email inyteractions with a poi could mean that the person is also a poi. This new feature, \"fraction_to_from_poi\", aggregates all email exchanges between the user and the pois already indicated, as a fraction of their total email exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of emails for Jim Piro to and from pois is 0.0135135135135\n",
      "The fraction of emails for Ken Lay to and from pois is 0.0322580645161\n"
     ]
    }
   ],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "## making a new feature which combines fraction of emails from and to poi and the user\n",
    "## function to derive fraction of messages from or to poi over all messages\n",
    "def email_fraction(email_poi, email_all):\n",
    "    fraction_of_messages = 0.\n",
    "    if email_poi == 'NaN' or email_poi == 'NaNNaN':\n",
    "        email_poi == 0\n",
    "    if email_all == 'NaN' or email_all == 'NaNNaN':\n",
    "        return fraction_of_messages\n",
    "    fraction_of_messages = float(email_poi)/float(email_all)\n",
    "    return fraction_of_messages\n",
    "## computing fraction of emails from and to poi\n",
    "## creating a new key:value pair to denote the total fraction of emails from and to pois\n",
    "for user in data_dict:   \n",
    "    all_emails = (data_dict[user][\"to_messages\"]) + (data_dict[user][\"from_messages\"])\n",
    "    all_poi = (data_dict[user][\"from_poi_to_this_person\"]) + (data_dict[user][\"from_this_person_to_poi\"])\n",
    "    fraction_to_from_poi = email_fraction(all_poi, all_emails)\n",
    "    data_dict[user][\"fraction_to_from_poi\"] = fraction_to_from_poi\n",
    "## checking whether the addition worked. \n",
    "print \"The fraction of emails for Jim Piro to and from pois is\", data_dict[\"PIRO JIM\"][\"fraction_to_from_poi\"]\n",
    "print \"The fraction of emails for Ken Lay to and from pois is\", data_dict[\"LAY KENNETH L\"][\"fraction_to_from_poi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 77 users with a positive fraction of emails to and from pois\n"
     ]
    }
   ],
   "source": [
    "## It worked!! Now is this even significant? Checking how many users had a positive interction with pois\n",
    "poi_email_fraction_count = 0\n",
    "for user in data_dict:\n",
    "    if data_dict[user][\"fraction_to_from_poi\"] != 0:\n",
    "        poi_email_fraction_count += 1\n",
    "print \"There are\", poi_email_fraction_count, \"users with a positive fraction of emails to and from pois\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the above output that more than half the users had some email exchange with pois. I am assuming that this interacton will be more impactful as a feature for poi identification when aggregated as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEPCAYAAABhkeIdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnFjMzCxX/x9uCpYRUnmKgAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108dd6ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Correlation with pois using a simple scatter plot to see if the the fraction_to_from_poi is distinct for pois and non-pois\n",
    "for user in data_dict:\n",
    "    poi = data_dict[user][\"poi\"]\n",
    "    fraction_to_from_poi = data_dict[user][\"fraction_to_from_poi\"]\n",
    "    if poi == True:\n",
    "        matplotlib.pyplot.scatter(poi, fraction_to_from_poi, color = \"b\")\n",
    "    else:\n",
    "        matplotlib.pyplot.scatter(poi, fraction_to_from_poi, color = \"r\")\n",
    "matplotlib.pyplot.xlabel(\"poi label\")\n",
    "matplotlib.pyplot.ylabel(\"Fraction of emails to and from pois\")\n",
    "matplotlib.pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot above shows that for most of the pois, the fraction is positive. But this is not very useful to understand whether the distribution is really different for pois and non- pois. Will see if this feature is significant using feature selection algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poi', 'salary', 'total_payments', 'exercised_stock_options', 'bonus', 'total_stock_value', 'long_term_incentive', 'shared_receipt_with_poi', 'from_this_person_to_poi', 'from_poi_to_this_person', 'restricted_stock', 'restricted_stock_deferred', 'deferred_income', 'director_fees', 'expenses', 'fraction_to_from_poi']\n"
     ]
    }
   ],
   "source": [
    "## Adding the new feature to features_list to see if that improves the decision tree\n",
    "features_list.append('fraction_to_from_poi')\n",
    "print features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New feature has been added to feature list, which is now ready to be incorporated into algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below. The cleaned data dictionary is stored as my_dataset\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction and preparation for cross validation\n",
    "Data was extracted with 'poi' as label and the rest as features. Used the StratifiedShuffleSplit recommended for this data set with 1000 fold random splitting iterations, as tyhe cross validation technique. Cross validation is the emans by which the classifier is thoroughly verified for application across the dataset and is a means to avoid both over and underfitting data. Though I am more used to splitting the data into discrete trainign and testing (usually a 70-30 split) points, I tried this as the cross validator. Later after algorithm tuning, the train- test- split was also tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "### Extract features and labels from my_dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)   \n",
    "folds = 1000\n",
    "cv = StratifiedShuffleSplit(labels, folds, random_state= 42)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    "I set up 3 classifiers- A simple Gaussian Naive Bayes, a KMeans to look at clustering effects and a Decision tree to enable a choice for further tuning and boosting. In all three, the features from the above created list were \n",
    "scaled using the MinMaxScaler for standardising as well as making them robust in the algorithm.\n",
    "\n",
    "### Evaluation\n",
    "Evaluation is the process of objectively assessing the performance of the classifier in predicting the outcome. Evaluation can be achieved by comparing many metrics. As evaluation metrics, accuracy, precision, recall and F1 scores were chosen, though the latter 3 were given more consideration. Due to the abundance of non- pois in the dataset, it is possible that, even with low precision and recall, the accuracy is still high. This was actually noticed in many trial runs of classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB classifier output:\n",
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', GaussianNB())])\n",
      "\tAccuracy: 0.32467\tPrecision: 0.15463\tRecall: 0.91000\tF1: 0.26434\tF2: 0.46029\n",
      "\tTotal predictions: 15000\tTrue positives: 1820\tFalse positives: 9950\tFalse negatives:  180\tTrue negatives: 3050\n",
      "\n",
      "Gaussian NB run time: 1.597 s\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "## Setting up 3 classifiers with feature scaling\n",
    "## Gaussian NB\n",
    "print \"Gaussian NB classifier output:\"\n",
    "NB_clf = Pipeline(steps =[('scaling', preprocessing.MinMaxScaler()), ('classifier', GaussianNB())])\n",
    "t0 = time()\n",
    "tester.test_classifier(NB_clf,my_dataset,features_list)\n",
    "print \"Gaussian NB run time:\", round(time ()- t0, 3),\"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just the selected features and feature scaling, Gaussian NB gave low precision but high recalland moderate F1 score. Accuracy was also low, though I do not want to consider it a key evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans classifier output:\n",
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=2, n_init=10,\n",
      "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
      "    verbose=0))])\n",
      "\tAccuracy: 0.51607\tPrecision: 0.13994\tRecall: 0.51100\tF1: 0.21971\tF2: 0.33392\n",
      "\tTotal predictions: 15000\tTrue positives: 1022\tFalse positives: 6281\tFalse negatives:  978\tTrue negatives: 6719\n",
      "\n",
      "KMeans run time: 17.278 s\n"
     ]
    }
   ],
   "source": [
    "## KMeans\n",
    "print \"KMeans classifier output:\"\n",
    "KM_clf = Pipeline(steps =[('scaling', preprocessing.MinMaxScaler()), ('classifier', KMeans(n_clusters = 2))])\n",
    "t0 = time()\n",
    "tester.test_classifier(KM_clf,my_dataset,features_list)\n",
    "print \"KMeans run time:\", round(time ()- t0, 3),\"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans with 2 clusters had a similar output as Gaussian NB, high recall but low precision but takes much longer to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree classifier output:\n",
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.80520\tPrecision: 0.27380\tRecall: 0.27900\tF1: 0.27637\tF2: 0.27794\n",
      "\tTotal predictions: 15000\tTrue positives:  558\tFalse positives: 1480\tFalse negatives: 1442\tTrue negatives: 11520\n",
      "\n",
      "Decision Tree run time: 1.751 s\n"
     ]
    }
   ],
   "source": [
    "## Decision tree\n",
    "print \"Decision Tree classifier output:\"\n",
    "DT_clf = Pipeline(steps =[('scaling', preprocessing.MinMaxScaler()), ('classifier', DecisionTreeClassifier())])\n",
    "t0 = time()\n",
    "tester.test_classifier(DT_clf,my_dataset,features_list)\n",
    "print \"Decision Tree run time:\", round(time ()- t0, 3),\"s\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree had higher accuracy than KMeans and Gaussian NB and had better precision and recall balance, with both tending towards 0.3 but not quite 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ranking\n",
      "1 expenses: 0.339068452381\n",
      "2 exercised_stock_options: 0.216469387755\n",
      "3 deferred_income: 0.107142857143\n",
      "4 fraction_to_from_poi: 0.0810267857143\n",
      "5 restricted_stock: 0.0666666666667\n",
      "6 from_this_person_to_poi: 0.0574829931973\n",
      "7 bonus: 0.0535714285714\n",
      "8 long_term_incentive: 0.0507936507937\n",
      "9 total_stock_value: 0.0277777777778\n",
      "10 director_fees: 0.0\n"
     ]
    }
   ],
   "source": [
    "## Getting feature rankings for all except label to see how they stack in terms of importance in the Decision Tree algorithm\n",
    "importances = DT_clf.steps[-1][1].feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print \"Feature Ranking\"\n",
    "for i in range(10):\n",
    "    print \"{} {}: {}\". format(i+1, features_list[1:][indices[i]], importances[indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new feature created, fraction_to_from_poi is 4th in the importances, it must be of some significance.  Proceeded to feature selection using KBest features. SelectKBest is a univariated fearure selection algorithm lets us select to top ranking features based on scores. I chose the Top 10 KBest features, hoping to streamline feature selection and identify the most impactful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top KBest features selected from features_list are\n",
      "                   Feature      Score\n",
      "                     bonus  24.815080\n",
      "       long_term_incentive  24.182899\n",
      "         total_stock_value  20.792252\n",
      "            total_payments  18.289684\n",
      "             director_fees  11.458477\n",
      "   shared_receipt_with_poi   9.922186\n",
      " restricted_stock_deferred   9.212811\n",
      "   exercised_stock_options   8.772778\n",
      "   from_this_person_to_poi   8.589421\n",
      "      fraction_to_from_poi   6.094173\n",
      "          restricted_stock   5.243450\n",
      "   from_poi_to_this_person   2.382612\n",
      "                  expenses   2.126328\n",
      "           deferred_income   0.065500\n"
     ]
    }
   ],
   "source": [
    "## Will use k-best algorithm to select features\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "select = SelectKBest(k = 10)\n",
    "## Removinf poi as a feature for k best\n",
    "features_list_k = features_list[1:]\n",
    "kbest_features = select.fit(features, labels)\n",
    "kbest_scores = kbest_features.scores_\n",
    "kbest_feature_score = zip(features_list_k[1:], kbest_scores)\n",
    "## converting to array for sorting by score\n",
    "selected_features = pandas.DataFrame(kbest_feature_score, columns =['Feature', 'Score'])\n",
    "selected_features = selected_features.sort_values(by ='Score', ascending = False)\n",
    "print \"The top KBest features selected from features_list are\"\n",
    "print selected_features.to_string(index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KBest features are remarkably different from the features ranked from the Decision tree. But, the new feature created, fraction_to_from_poi, is still part of the top 10 features, ranked at 10. The Decision Tree was re- run with KBest fetaures, using the pipeline with feature scaling and the tester's cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree classifier with KBest features output:\n",
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.80567\tPrecision: 0.27363\tRecall: 0.27650\tF1: 0.27506\tF2: 0.27592\n",
      "\tTotal predictions: 15000\tTrue positives:  553\tFalse positives: 1468\tFalse negatives: 1447\tTrue negatives: 11532\n",
      "\n",
      "Decision Tree run time: 1.713 s\n"
     ]
    }
   ],
   "source": [
    "## Rerunning Decision tree with KBest features after updating the features_list\n",
    "features_list = ['poi', 'bonus', 'long_term_incentive', 'total_stock_value', 'total_payments', 'director_fees', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'exercised_stock_options', 'from_this_person_to_poi', 'fraction_to_from_poi', 'restricted_stock', 'from_poi_to_this_person', 'expenses', 'deferred_income']\n",
    "print \"Decision Tree classifier with KBest features output:\"\n",
    "DT_K_clf = Pipeline(steps =[('scaling', preprocessing.MinMaxScaler()), ('classifier', DecisionTreeClassifier(random_state = None))])\n",
    "t0 = time()\n",
    "tester.test_classifier(DT_K_clf,my_dataset,features_list)\n",
    "print \"Decision Tree run time:\", round(time ()- t0, 3),\"s\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KBest features coupled with feature scaling do not alter the outcome of the Decision Tree algorithm. The algorithms need tuning in terms of classifier parameters. Chose to tune Gaussian NB and Decision Tree , dimensionality reduction, GridSearch CV and boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the classifier\n",
    "Tuning the classifier is the process of choosing the parameters that lead to the best performance of the classifier. Tuning can be done in many ways. I chose to reduce dimensions using PCA and use GridSerach CV which lets us mix and match parameters in a deliberate matrix, thereby arriving at the best parameters for a given classifier.\n",
    "Gaussian NB cannot be really tuned by altering parameters, only feature selection and PCA were employed to tune it. On the other hand, Decision Tree was tuned with GridSearchCV, with several choices of core performance parameters. The best parameters were then chosen to be boosted by AdaBoost which was in turn tuned by GridSearchCV. For all pipelines, features were scaled and F1 scores were used as the evaluation metric to assess the performce of the classifier, since F1 score is the weighted average of precision and recall. Cross validation was achieved by use of the tester's data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB classifier with PCA output:\n",
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('gaussian', GaussianNB())])\n",
      "\tAccuracy: 0.82153\tPrecision: 0.34855\tRecall: 0.38950\tF1: 0.36789\tF2: 0.38056\n",
      "\tTotal predictions: 15000\tTrue positives:  779\tFalse positives: 1456\tFalse negatives: 1221\tTrue negatives: 11544\n",
      "\n",
      "Gaussian NB with PCA run time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "## Tuning Gaussian NB with PCA to reduce dimensions. Grid Search CV has no parameters to set for NB\n",
    "print \"Gaussian NB classifier with PCA output:\"\n",
    "NB_PCA_clf = Pipeline(steps =[('scaling', preprocessing.MinMaxScaler()),('pca', PCA(copy = True, n_components = 0.95, whiten = False)), ('gaussian', GaussianNB())])\n",
    "t0 = time()\n",
    "tester.test_classifier(NB_PCA_clf,my_dataset,features_list)\n",
    "print \"Gaussian NB with PCA run time:\", round(time ()- t0, 3),\"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA drastically improves the performance of the Gaussian NB algorithm!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree classifier with PCA output:\n",
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('pca', PCA(copy=True, n_components=0.95, whiten=False)), ('classifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'))])\n",
      "\tAccuracy: 0.79153\tPrecision: 0.21439\tRecall: 0.21150\tF1: 0.21294\tF2: 0.21207\n",
      "\tTotal predictions: 15000\tTrue positives:  423\tFalse positives: 1550\tFalse negatives: 1577\tTrue negatives: 11450\n",
      "\n",
      "Decision Tree run time: 2.232 s\n"
     ]
    }
   ],
   "source": [
    "## Using PCA to reduce dimensions for decision tree\n",
    "print \"Decision Tree classifier with PCA output:\"\n",
    "DT_PCA_clf = Pipeline(steps =[('scaling', preprocessing.MinMaxScaler()), ('pca', PCA(copy = True, n_components = 0.95, whiten = False)), ('classifier', DecisionTreeClassifier(random_state = None))])\n",
    "t0 = time()\n",
    "tester.test_classifier(DT_PCA_clf,my_dataset,features_list)\n",
    "print \"Decision Tree run time:\", round(time ()- t0, 3),\"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the Decision Tree algorithm falls with PCA. Will proceed to tune this algorithm to find the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree tuning by GridSearchCV\n",
      "Best parameters: {'classifier__max_features': None, 'classifier__min_samples_split': 4, 'classifier__class_weight': {0: 0.5, 1: 1}, 'classifier__splitter': 'best', 'classifier__min_weight_fraction_leaf': 0.1, 'classifier__max_depth': 3, 'classifier__criterion': 'gini'} and Best score: 0.337226334776\n"
     ]
    }
   ],
   "source": [
    "## Tuning Decision Tree with Grid Search CV\n",
    "param_grid = [{'classifier__min_samples_split': [2, 4],\n",
    "          'classifier__max_depth': [None, 3, 8],\n",
    "          'classifier__max_features': [None, 'auto', 'log2'],\n",
    "          'classifier__criterion': ['gini', 'entropy'],\n",
    "          'classifier__splitter': ['best', 'random'],\n",
    "          'classifier__min_weight_fraction_leaf': [0, 0.1],\n",
    "          'classifier__class_weight': [{1: 1, 0: 0.5}, {1: 0.8, 0: 0.3}]\n",
    "    }]\n",
    "DT_grid_clf = Pipeline(steps =[('scaling', preprocessing.MinMaxScaler()), ('classifier', DecisionTreeClassifier(random_state = None))])\n",
    "DT_grid = GridSearchCV(DT_grid_clf, param_grid, cv=cv, scoring = 'f1')\n",
    "DT_grid.fit(features, labels)\n",
    "print \"Decision Tree tuning by GridSearchCV\"\n",
    "print \"Best parameters:\", DT_grid.best_params_, \"and Best score:\", DT_grid.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the best combination of parameters for the Decision Tree algorithm have been found, will re- run the algorithm using the tester with the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', DecisionTreeClassifier(class_weight={0: 0.5, 1: 1}, criterion='gini',\n",
      "            max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.1, presort=False, random_state=None,\n",
      "            splitter='best'))])\n",
      "\tAccuracy: 0.82960\tPrecision: 0.36479\tRecall: 0.37500\tF1: 0.36982\tF2: 0.37291\n",
      "\tTotal predictions: 15000\tTrue positives:  750\tFalse positives: 1306\tFalse negatives: 1250\tTrue negatives: 11694\n",
      "\n",
      "Decision Tree run time: 1.804 s\n"
     ]
    }
   ],
   "source": [
    "## Running decision tree algorithm with best parameters\n",
    "DT_best_clf = DT_K_clf.set_params(**DT_grid.best_params_)\n",
    "t0 = time()\n",
    "tester.test_classifier(DT_best_clf,my_dataset,features_list)\n",
    "print \"Decision Tree run time:\", round(time ()- t0, 3),\"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics are much better now, but may be improved even more by boosting. Adaboost has the capacity to iteratively boost the contribution of the weker features, which will give improved metrics, if it works. Using the Decision Tree with best parameters as a base estimator, will cinfigure the best parameters, such as number of estimators and the learning rate for AdaBoost using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###  Will boost this with Adaboost to see if metrics are better\n",
    "DT_best_params = DT_best_clf.steps[-1][1].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost tuning of Decision Tree by GridSearchCV\n",
      "Best parameters: {'classifier__learning_rate': 2, 'classifier__base_estimator': DecisionTreeClassifier(class_weight={0: 0.5, 1: 1}, criterion='gini',\n",
      "            max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.1, presort=False, random_state=None,\n",
      "            splitter='best'), 'classifier__n_estimators': 5} and Best score: 0.22113015873\n"
     ]
    }
   ],
   "source": [
    "## Using these parameters, do a grid serachCV for best Adaboost params\n",
    "AB_params = [{'classifier__base_estimator': [DecisionTreeClassifier(**DT_best_params)],\n",
    "'classifier__n_estimators': [5, 10, 50, 70],\n",
    "'classifier__learning_rate': [0.5, 1, 2]\n",
    "}]\n",
    "AB_grid_clf = Pipeline(steps = [('scaling', preprocessing.MinMaxScaler()), ('classifier', AdaBoostClassifier(random_state = None))])\n",
    "AB_grid = GridSearchCV(AB_grid_clf, AB_params, cv = cv, scoring = 'f1')\n",
    "AB_grid.fit(features, labels)\n",
    "print \"AdaBoost tuning of Decision Tree by GridSearchCV\"\n",
    "print \"Best parameters:\", AB_grid.best_params_, \"and Best score:\", AB_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('classifier', AdaBoostClassifier(algorithm='SAMME.R',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight={0: 0.5, 1: 1}, criterion='gini',\n",
      "            max_depth=3, max_features=None, max_leaf_nodes=None,\n",
      "            min_sa...=None,\n",
      "            splitter='best'),\n",
      "          learning_rate=2, n_estimators=5, random_state=None))])\n",
      "\tAccuracy: 0.80180\tPrecision: 0.24961\tRecall: 0.24250\tF1: 0.24601\tF2: 0.24389\n",
      "\tTotal predictions: 15000\tTrue positives:  485\tFalse positives: 1458\tFalse negatives: 1515\tTrue negatives: 11542\n",
      "\n",
      "AdaBoost run time: 20.822 s\n"
     ]
    }
   ],
   "source": [
    "## Running decision tree algorithm with best features and boosting\n",
    "AB_best_clf = AB_grid_clf.set_params(**AB_grid.best_params_)\n",
    "t0 = time()\n",
    "tester.test_classifier(AB_best_clf,my_dataset,features_list)\n",
    "print \"AdaBoost run time:\", round(time ()- t0, 3),\"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboosting did not add anything to the Decision tree. It is possible that other tuning mechanisms may yield better metrics as well as other base estimators. \n",
    "\n",
    "### Best Classifier\n",
    "At this point, I will choose the GridSearch CV tuned Decision Tree as my best classifier based on its precision, recall and F1 scores, which were similar to the next best PCA- transformed Gaussian NB classifier. Summarising both,\n",
    "\n",
    "Performance of Decision Tree tuned by Grid Search \n",
    "Accuracy: 0.82960\tPrecision: 0.36479\tRecall: 0.37500\t F1: 0.36982\n",
    "Performance of Gaussian NB transformed by PCA\n",
    "Accuracy: 0.82153\tPrecision: 0.34855\tRecall: 0.38950\t F1: 0.36789\n",
    "\n",
    "For cross validation, I tried train_test_split to withhold 30% of the data for testing and used the rest for training. The tester uses StratifiedShufflesplit (folds=1000) for cross validation. Both were similar, though the tester's cross validation yielded slightly better metrics. \n",
    "\n",
    "In this project, evaluation was done by comparing precision, recall and F1 scores as standard metrics. Accuracy is a measure of correct poi identifications, which given the high non-poi : poi ratio, will be high and hence is not a very meaningful metric. Precision, which is the measure of true positives identified among all positives, gives a measure of identification of ture pois. Here, it means that 36.5% of pois identified are true pois. Whereas Recall is a measure of false negatives identified. Here, a recall of 0.375, means there are some false negatives, who should be identified as true positives by the algorithm. The F1 score is  mid- range  reflecting both these metrics. To summarise, some true pois will not be identified in this algorithm. But the pois identified can be reliably considered true pois and an investigation of their records will be useful in leading to the roots of the fraud. But ethical use of data and the outcome of the learning process ought to be ensured too.\n",
    "\n",
    "### Conclusion\n",
    "My take on the dataset and what I learned by applying machine learning techniques in identifying pois is that\n",
    "1. A dataset such as this, with a bunch of distinct aspects- financial, stock and email exchange, has unique qualities that lend it to multiple channels of investigation. \n",
    "2. But, given the extensive nature of the data, the information is still not complete (a high incidence of NaNs) or realated enough to be applicable across the board for identifying pois. Except for some individuals, who are clearly pois, others are difficult to predict  following the same trend. It is this grey area that gives us moderate precision and recall. Some text learning could have helped, which I will continue doing on this dataset.\n",
    "3. In the choice between precision and recall for this dataset, I would go with a balance of both. While false negative identification can lead to the guilty slippig away, false positive identification will lead to the innocent being put through scrutiny which is both detrimental to their lives as well as unethical. So, even though some positives are missed, as long as negatives are not identified as pois at high rates, the algotrithm is satisfactory for starters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* https://help.github.com/articles/getting-started-with-writing-and-formatting-on-github/\n",
    "* https://ipython.org/ipython-doc/1/interactive/notebook.html\n",
    "* http://scikit-learn.org/stable/documentation.html\n",
    "* https://jaycode.github.io/enron/identifying-fraud-from-enron-email.html\n",
    "* \"The Master Algorithm\" by Pedro Domingos\n",
    "* https://github.com/allanbreyes/udacity-data-science\n",
    "* https://github.com/thuyquach/Identify-Fraud-from-Enron-Email\n",
    "* http://machinelearningmastery.com/quick-and-dirty-data-analysis-with-pandas/\n",
    "* https://www.dataquest.io/blog/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
